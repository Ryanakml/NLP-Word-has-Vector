{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13015ff3-c1e1-4a91-a12f-3815fdec1966",
   "metadata": {},
   "source": [
    "# Word to Vector\n",
    "\n",
    "![](https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0077ea-6d53-477c-95b9-1034f1fbf4d5",
   "metadata": {},
   "source": [
    "**yap**, its basically how we turn word to numbers, so computer will understand it.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1372/format:webp/1*YXy_Txtmtttw85Vv05JdRQ.jpeg)\n",
    "\n",
    "that is how basically text are converted into vectors.\n",
    "\n",
    "however, counting or giving each word a number didnt do anything, the word has meaning, and this is what we want computer to understand.\n",
    "\n",
    "how?, simply just giving the weight to word to track how valueable a word is.\n",
    "\n",
    "we often use TF-IDF (Term Frequency-Inverse Term Frequency), will weighting a word to define important feature within text or document or corpus.\n",
    "\n",
    "More weight means more important, for example how many word appear in corpus, more appear more weight more important (Term Frequency).\n",
    "\n",
    "ex : \n",
    "\n",
    "Corpus : \"Dennis loves programming, he programs all day, he will be a world class programmer one day\"\n",
    "\n",
    "tokenized + count : program(3), day(2), .. ,else(1).\n",
    "\n",
    "TF = (number of times word appear in corpus)/(total number of words appear in corpus).\n",
    "\n",
    "however, if another corpus have a lot of same word appear, that word will less meaning and cant be a feature.\n",
    "\n",
    "so solve it, we use IDF (Inverse Term Freqency), what you expect?, its basically a reverse, it measure how much information the word provides, weather the term is common(TF High) or rare(TF Low) accross all corpus.\n",
    "\n",
    "IDF = log(total number of corpus)/(number of corpus with the term t in it).\n",
    "\n",
    "so.. TF-IDF = TF*IDF. \n",
    "\n",
    "basically using TF-IDF can get most important feature from corpus with weight.\n",
    "\n",
    "lets see the different between just count and TF-IDF Vectorizer.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1210/format:webp/1*-aZMvSiEXTbxIpHcvnIINg.jpeg)\n",
    "\n",
    "TF-IDF will give us value of a word based on how important a word is in our corpuses. for example also is meaningless when we see corpus 1, but it is very valueable or informative in second corpus.\n",
    "\n",
    "blablabla, we need the code jimmy. action is number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518a4b12-5177-4e01-a6af-3bbc7a4ffecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>love</th>\n",
       "      <th>programming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love programming</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming also love</th>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           also      love  programming\n",
       "love programming       0.000000  0.707107     0.707107\n",
       "programming also love  0.704909  0.501549     0.501549"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "docs = [\"love programming\", \"programming also love\"]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer1 = TfidfVectorizer(sublinear_tf=True, max_df=1.0)\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow1 = vectorizer1.fit_transform(docs)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer1.get_feature_names_out()\n",
    "\n",
    "# Create index for rows\n",
    "corpus_index = [n for n in docs]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(bow1.todense(), index=corpus_index, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138763c-1100-45df-92d2-38395e36117d",
   "metadata": {},
   "source": [
    "terminology alert, if we take one word as a feature thats called unigram. if we take two word as a feature, thats called Bigram, and if three its Trigram. you see the pattern, but lets representing it as n-gram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6062952-0db8-4daf-aba1-73cd71f66da2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           also  also love      love  love programming  \\\n",
      "love programming       0.000000   0.000000  0.501549          0.704909   \n",
      "programming also love  0.499221   0.499221  0.355200          0.000000   \n",
      "\n",
      "                       programming  programming also  \n",
      "love programming          0.501549          0.000000  \n",
      "programming also love     0.355200          0.499221  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "docs = [\"love programming\", \"programming also love\"]\n",
    "\n",
    "# TF-IDF with both unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1.0, ngram_range=(1, 2))\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(tfidf_matrix.todense(), index=docs, columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5445c3a-25bc-4215-b2cd-b739666c3c49",
   "metadata": {},
   "source": [
    "**The point is** the value of a token or feature or bigram or unigram is representing how important that feature is in a corpus. since tf telling how often it appears in a corpus and idf tells how unique it is accross all corpuses. meaning if the value is high that feature or token is frequent in that corpus and rare in other.\n",
    "\n",
    "This value make more sense or add much meaning than just counting it. that table is called sparse matrix. now this matrix is what we give model to train with. Sparse Matrix = X_Train (Input)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf1d27-14f9-4b8f-add2-dc03275d8e08",
   "metadata": {},
   "source": [
    "Sounds like a solution right?. however, there is a problem, say we have \n",
    "\n",
    "c1 : the movie is great.\n",
    "\n",
    "c2 : goo is the movie.\n",
    "\n",
    "with TF-IDF the value of a word is all the same accross corpus, even if it is not make any sense. the meaning isnt defined greatly here.\n",
    "\n",
    "The solution is using word2vec. same goal, we want to representing a word into numerical values that computer can understand 'well'.\n",
    "\n",
    "Lets start with using 'one-hot' method to convert word into sparse representation. we will give value a word as 1 and else will 0 for representing that word is important. now say for our c1, it will have 4 vector size. but how about if our corpus is a dictionary which have 10000 word, how tf we are gonna compute that 10000 vector size.\n",
    "\n",
    "Word2vec again comes to take care mainly two things here\n",
    "\n",
    "1. convert this high dimentional vector (10000 sized) into lower dimentional vector size (ex. 100). this conversion is called word embedding.\n",
    "\n",
    "2. Mantains the word value importance or context (meaning). Meaning can be defined within two methods:\n",
    "\n",
    "    - Bag of Words, predict a word based on surrounding words. ex 'i love pizza' -> x = ['i', 'love'] y = ['pizza'].\n",
    "    - Skip-Gram, take a word and predict the surrounding. ex 'i love pizza' -> x = ['love'], y = ['i', 'pizza'].\n",
    "\n",
    "Now, how word2vec works?\n",
    "1. Take a neural nets with one input layer, one hidden layer and one output layer.\n",
    "   \n",
    "- ![](https://s3.eu-west-1.amazonaws.com/redsys-prod/articles/e375fac2371981c259ad7979/images/image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f00d95-4612-4934-9703-464703a59f63",
   "metadata": {},
   "source": [
    "see, we windowing 4 word and we try to predict the nearby word. \n",
    "\n",
    "corpus = '.... spike with pain and music is my aeroplane ....'\n",
    "\n",
    "4 window = 'music is my aeroplane'\n",
    "\n",
    "x = ['aeroplane']\n",
    "\n",
    "y = ['music', 'is', 'my']\n",
    "\n",
    "the output is called word embeddings. and this representing how computer know the context when we talk about aeroplane, since it can predict what words correlated to aeroplane.\n",
    "\n",
    "one last note is since the hidden layer and output layer has so much weights in it, when we try to calculate the xi = 1, the other xi = 0 will be dropout and the neuron didnt active meaning the weight didnt being calculate.\n",
    "\n",
    "Lets Recap, word2vec is a neural network training for all the word in our dictionary to get weight or parameter or vectors. it will have a word embeddings for every word in the dicitonary or corpus.\n",
    "\n",
    "well, with this we can get similar vector for plural word, or has similar meaning, for example 'great', 'good', 'awesome' ... from a really big corpus. this is how we want to make computer understand that word has same meaning or context. imagine if every word has a correlation one another, it will has meaning with more contextual.\n",
    "\n",
    "okey thats all, whats next, hmm....\n",
    "\n",
    "![](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fb193-3a60-40d5-bb54-c12123c77fd1",
   "metadata": {},
   "source": [
    "oh yeah, the code, lets end the bla bla and implement what we already learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b5d5ae-422e-4111-aff2-ed835284c160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('corona.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c0d522d-0f28-4af8-9e63-b1fa5a1ae417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trending new yorkers encounter empty supermarket shelves pictured wegmans in brooklyn soldout online grocers foodkick maxdelivery as coronavirusfearing shoppers stock up',\n",
       " 'when i couldnt find hand sanitizer at fred meyer i turned to amazon but for a pack of purellcheck out how coronavirus concerns are driving up prices',\n",
       " 'find out how you can protect yourself and loved ones from coronavirus',\n",
       " 'panic buying hits newyork city as anxious shoppers stock up on foodampmedical supplies after healthcare worker in her s becomes bigapple st confirmed coronavirus patient or a bloomberg staged event qanon qanon qanon election cdc',\n",
       " 'toiletpaper dunnypaper coronavirus coronavirusaustralia coronavirusupdate covid news corvid newsmelb dunnypapergate costco one week everyone buying baby milk powder the next everyone buying up toilet paper']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|<url>', '', text)  # hapus url\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # hapus simbol kecuali huruf dan spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "df = df['OriginalTweet'][:10]\n",
    "normalized_sentences = [normalize_text(text) for text in df]\n",
    "normalized_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "570e48b4-9b6c-4fe8-91f8-ea87c5a139c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count :  [('the', 9), ('a', 7), ('coronavirus', 7), ('in', 5), ('i', 5)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# sentences to words and count\n",
    "words = \" \".join(normalized_sentences).split()\n",
    "count = collections.Counter(words).most_common()\n",
    "\n",
    "print(\"Word Count : \", count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "797cb12e-1d05-42e5-8026-cd209afa8731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'a': 1,\n",
       " 'coronavirus': 2,\n",
       " 'in': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'up': 6,\n",
       " 'for': 7,\n",
       " 'of': 8,\n",
       " 'last': 9,\n",
       " 'as': 10,\n",
       " 'hand': 11,\n",
       " 'sanitizer': 12,\n",
       " 'at': 13,\n",
       " 'out': 14,\n",
       " 'how': 15,\n",
       " 'are': 16,\n",
       " 'prices': 17,\n",
       " 'buying': 18,\n",
       " 'qanon': 19,\n",
       " 'covid': 20,\n",
       " 'do': 21,\n",
       " 'is': 22,\n",
       " 'supermarket': 23,\n",
       " 'shoppers': 24,\n",
       " 'stock': 25,\n",
       " 'when': 26,\n",
       " 'find': 27,\n",
       " 'to': 28,\n",
       " 'or': 29,\n",
       " 'everyone': 30,\n",
       " 'masks': 31,\n",
       " 'have': 32,\n",
       " 'amp': 33,\n",
       " 'should': 34,\n",
       " 'no': 35,\n",
       " 'pharmacies': 36,\n",
       " 'trending': 37,\n",
       " 'new': 38,\n",
       " 'yorkers': 39,\n",
       " 'encounter': 40,\n",
       " 'empty': 41,\n",
       " 'shelves': 42,\n",
       " 'pictured': 43,\n",
       " 'wegmans': 44,\n",
       " 'brooklyn': 45,\n",
       " 'soldout': 46,\n",
       " 'online': 47,\n",
       " 'grocers': 48,\n",
       " 'foodkick': 49,\n",
       " 'maxdelivery': 50,\n",
       " 'coronavirusfearing': 51,\n",
       " 'couldnt': 52,\n",
       " 'fred': 53,\n",
       " 'meyer': 54,\n",
       " 'turned': 55,\n",
       " 'amazon': 56,\n",
       " 'but': 57,\n",
       " 'pack': 58,\n",
       " 'purellcheck': 59,\n",
       " 'concerns': 60,\n",
       " 'driving': 61,\n",
       " 'can': 62,\n",
       " 'protect': 63,\n",
       " 'yourself': 64,\n",
       " 'and': 65,\n",
       " 'loved': 66,\n",
       " 'ones': 67,\n",
       " 'from': 68,\n",
       " 'panic': 69,\n",
       " 'hits': 70,\n",
       " 'newyork': 71,\n",
       " 'city': 72,\n",
       " 'anxious': 73,\n",
       " 'on': 74,\n",
       " 'foodampmedical': 75,\n",
       " 'supplies': 76,\n",
       " 'after': 77,\n",
       " 'healthcare': 78,\n",
       " 'worker': 79,\n",
       " 'her': 80,\n",
       " 's': 81,\n",
       " 'becomes': 82,\n",
       " 'bigapple': 83,\n",
       " 'st': 84,\n",
       " 'confirmed': 85,\n",
       " 'patient': 86,\n",
       " 'bloomberg': 87,\n",
       " 'staged': 88,\n",
       " 'event': 89,\n",
       " 'election': 90,\n",
       " 'cdc': 91,\n",
       " 'toiletpaper': 92,\n",
       " 'dunnypaper': 93,\n",
       " 'coronavirusaustralia': 94,\n",
       " 'coronavirusupdate': 95,\n",
       " 'news': 96,\n",
       " 'corvid': 97,\n",
       " 'newsmelb': 98,\n",
       " 'dunnypapergate': 99,\n",
       " 'costco': 100,\n",
       " 'one': 101,\n",
       " 'week': 102,\n",
       " 'baby': 103,\n",
       " 'milk': 104,\n",
       " 'powder': 105,\n",
       " 'next': 106,\n",
       " 'toilet': 107,\n",
       " 'paper': 108,\n",
       " 'remember': 109,\n",
       " 'time': 110,\n",
       " 'paid': 111,\n",
       " 'gallon': 112,\n",
       " 'regular': 113,\n",
       " 'gas': 114,\n",
       " 'los': 115,\n",
       " 'angelesprices': 116,\n",
       " 'pump': 117,\n",
       " 'going': 118,\n",
       " 'down': 119,\n",
       " 'look': 120,\n",
       " 'impacting': 121,\n",
       " 'pm': 122,\n",
       " 'abc': 123,\n",
       " 'voting': 124,\n",
       " 'age': 125,\n",
       " 'supertuesday': 126,\n",
       " 'drtedros': 127,\n",
       " 'we': 128,\n",
       " 'cant': 129,\n",
       " 'stop': 130,\n",
       " 'without': 131,\n",
       " 'protecting': 132,\n",
       " 'healthworkers': 133,\n",
       " 'surgical': 134,\n",
       " 'increased': 135,\n",
       " 'sixfold': 136,\n",
       " 'n': 137,\n",
       " 'respirators': 138,\n",
       " 'more': 139,\n",
       " 'than': 140,\n",
       " 'trebled': 141,\n",
       " 'gowns': 142,\n",
       " 'cost': 143,\n",
       " 'twice': 144,\n",
       " 'muchdrtedros': 145,\n",
       " 'hi': 146,\n",
       " 'twitter': 147,\n",
       " 'am': 148,\n",
       " 'pharmacist': 149,\n",
       " 'sell': 150,\n",
       " 'living': 151,\n",
       " 'any': 152,\n",
       " 'exists': 153,\n",
       " 'like': 154,\n",
       " 'it': 155,\n",
       " 'sold': 156,\n",
       " 'fuck': 157,\n",
       " 'everywhere': 158,\n",
       " 'be': 159,\n",
       " 'worried': 160,\n",
       " 'use': 161,\n",
       " 'soap': 162,\n",
       " 'visit': 163,\n",
       " 'twenty': 164,\n",
       " 'looking': 165,\n",
       " 'bottle': 166,\n",
       " 'full': 167,\n",
       " 'sick': 168,\n",
       " 'people': 169,\n",
       " 'anyone': 170,\n",
       " 'been': 171,\n",
       " 'over': 172,\n",
       " 'few': 173,\n",
       " 'days': 174,\n",
       " 'went': 175,\n",
       " 'my': 176,\n",
       " 'normal': 177,\n",
       " 'shop': 178,\n",
       " 'night': 179,\n",
       " 'sight': 180,\n",
       " 'that': 181,\n",
       " 'greeted': 182,\n",
       " 'me': 183,\n",
       " 'barmy': 184,\n",
       " 'btw': 185,\n",
       " 'whats': 186,\n",
       " 'so': 187,\n",
       " 'special': 188,\n",
       " 'about': 189,\n",
       " 'tinned': 190,\n",
       " 'tomatoes': 191,\n",
       " 'dublin': 192}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionaries\n",
    "\n",
    "unique_words = [i[0] for i in count]\n",
    "dic = {w : i for i, w in enumerate(unique_words)}\n",
    "win_size = len(dic)\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9e8d560-1088-4ad4-86d2-21513418612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data :  [37, 38, 39, 40, 41, 23, 42, 43, 44, 3] ['trending', 'new', 'yorkers', 'encounter', 'empty', 'supermarket', 'shelves', 'pictured', 'wegmans', 'in']\n"
     ]
    }
   ],
   "source": [
    "data = [dic[word] for word in words]\n",
    "print('Sample Data : ', data[:10], words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8786e-34b3-46d3-9619-ef716b31d22d",
   "metadata": {},
   "source": [
    "**Now lets start using word2vec ... we can use bag of words and skip gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12c8f0c9-f163-4c52-8c0d-9b98758c5a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context pairs ranks ids:  [[[37, 39], 38], [[38, 40], 39], [[39, 41], 40], [[40, 23], 41], [[41, 42], 23]]\n",
      "\n",
      "Context pairs words :  [[['trending', 'yorkers'], 'new'], [['new', 'encounter'], 'yorkers'], [['yorkers', 'empty'], 'encounter'], [['encounter', 'supermarket'], 'empty'], [['empty', 'shelves'], 'supermarket']]\n"
     ]
    }
   ],
   "source": [
    "cbow = []\n",
    "\n",
    "for i in range(1, len(data)-1):\n",
    "    cbow.append([[data[i-1], data[i+1]], data[i]])\n",
    "\n",
    "\n",
    "print(\"Context pairs ranks ids: \", cbow[:5])\n",
    "print()\n",
    "\n",
    "cbow_words = []\n",
    "\n",
    "for i in range(1, len(words)-1):\n",
    "    cbow_words.append([[words[i-1], words[i+1]], words[i]])\n",
    "\n",
    "print('Context pairs words : ', cbow_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd890673-3891-4228-b0d0-3097f2804d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip-gram pairs :  [[38, 37], [38, 39], [39, 38], [39, 40], [40, 39]]\n",
      "\n",
      "skip-gram pairs words :  [['new', 'trending'], ['new', 'yorkers'], ['yorkers', 'new'], ['yorkers', 'encounter'], ['encounter', 'yorkers']]\n"
     ]
    }
   ],
   "source": [
    "skip_gram = []\n",
    "\n",
    "for _ in cbow:\n",
    "    skip_gram.append([_[1], _[0][0]])\n",
    "    skip_gram.append([_[1], _[0][1]])\n",
    "\n",
    "print('skip-gram pairs : ', skip_gram[:5])\n",
    "print()\n",
    "\n",
    "skip_gram_words = []\n",
    "for _ in cbow_words:\n",
    "    skip_gram_words.append([_[1], _[0][0]])\n",
    "    skip_gram_words.append([_[1], _[0][1]])\n",
    "\n",
    "print('skip-gram pairs words : ', skip_gram_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b23ec5-48e5-4010-b707-e263b7235559",
   "metadata": {},
   "source": [
    "**now lets use it as traning datasets....**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73cc3608-0e9f-4977-b828-4081d702f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches (x,y) :  ([2, 77, 173], [[145], [76], [9]])\n",
      "\n",
      "Batches with words:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id_to_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches with words:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batch, Y_batch):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCenter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mid_to_word\u001b[49m[x]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) -> Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_to_word[y[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id_to_word' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batch(size):\n",
    "    assert size<len(skip_gram)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    rdm = np.random.choice(range(len(skip_gram)), size, replace=False)\n",
    "\n",
    "    for r in rdm:\n",
    "        X.append(skip_gram[r][0])\n",
    "        Y.append([skip_gram[r][1]])\n",
    "    return X,Y\n",
    "\n",
    "X_batch, Y_batch = get_batch(3)\n",
    "\n",
    "print(\"Batches (x,y) : \", get_batch(3))\n",
    "print()\n",
    "\n",
    "print(\"Batches with words:\")\n",
    "for x, y in zip(X_batch, Y_batch):\n",
    "    print(f\"Center: {skip_gram_words\n",
    "    [x]} ({x}) -> Context: {id_to_word[y[0]]} ({y[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6802f07-bbe3-49a9-abf9-af6ad1dba50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
